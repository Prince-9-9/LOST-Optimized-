# Copyright 2021 - Valeo Comfort and Driving Assistance - Oriane SimÃ©oni @ valeo.ai
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import torch
import scipy
import scipy.ndimage

import numpy as np
from datasets import bbox_iou

def optimized_seed_selection(features, attention_maps, args):
    """
    The function `optimized_seed_selection` selects a seed based on the features, attention maps, and
    coverage scores using adaptive seed selection logic.
    
    :param features: Features are the input features extracted from a neural network, typically
    representing the activations of intermediate layers or specific regions of an image. In the context
    of the provided code snippet, features are used for seed selection based on coverage and other
    criteria
    :param attention_maps: Attention maps from the model, with shape [B, H, N, N], where B is the batch
    size, H is the number of attention heads, and N is the sequence length
    :param args: The `args` parameter in the `optimized_seed_selection` function is used to pass any
    additional arguments or configurations that may be needed for the seed selection process. These
    arguments could include hyperparameters, settings, or any other values that affect how the seed is
    selected based on the features and attention maps provided
    :return: The function `optimized_seed_selection` returns the selected seed for further processing
    based on the input features, attention maps, and arguments.
    """
    # 1. Use CLS token attention to analyze coverage
    def analyze_object_coverage(attention_maps):
        """
        The function `analyze_object_coverage` calculates the coverage score based on the average
        attention across heads in the given attention maps.
        
        :param attention_maps: The `attention_maps` parameter is a tensor representing the attention
        maps generated by a transformer model. It has a shape of [B, H, N, N], where:
        :return: The function `analyze_object_coverage` returns the coverage score calculated based on
        the input attention maps.
        """
        # Average attention across heads
        avg_attention = attention_maps.mean(dim=1)  # [B, N, N]
        coverage_score = avg_attention[:, 0, 1:].mean(dim=1)  # Use CLS token attention
        return coverage_score

    # 2. Adaptive seed selection based on coverage
    def select_seed_adaptive(features, degrees, coverage_score):
        """
        The function `select_seed_adaptive` calculates scores based on distinctiveness and degrees,
        prioritizing distinctiveness for large objects, and returns the index of the minimum score.
        
        :param features: The `features` parameter in the `select_seed_adaptive` function represents the
        features of the objects being considered for selection. These features are used to calculate the
        distinctiveness and similarity between objects in order to determine the seed for further
        processing
        :param degrees: Degrees represents the degrees of the nodes in a graph. It is a measure of the
        connectivity of a node, indicating the number of edges connected to that node
        :param coverage_score: The `coverage_score` parameter in the `select_seed_adaptive` function is
        used to determine whether the large object case or the normal case should be considered. If the
        `coverage_score` is greater than 0.7, the function considers it as a large object case and
        calculates the scores based
        :return: The function `select_seed_adaptive` returns the index of the minimum value in the
        `scores` array.
        """
        if coverage_score > 0.7:  # Large object case
            # Use feature distinctiveness more than degree
            distinctiveness = 1 - torch.cosine_similarity(
                features.unsqueeze(1), 
                features.unsqueeze(0), 
                dim=2
            ).mean(dim=1)
            scores = distinctiveness * 0.7 + (degrees / degrees.max()) * 0.3
        else:  # Normal case
            scores = degrees
        
        return scores.argmin()

    # Main selection logic
    with torch.no_grad():
        coverage = analyze_object_coverage(attention_maps)
        degrees = compute_patch_degrees(features)
        seed = select_seed_adaptive(features, degrees, coverage)
        
    return seed


def enhanced_instance_separation(features, initial_seed, adjacency, args):
    """
    The function `enhanced_instance_separation` takes features, initial seed, adjacency matrix, and
    arguments to separate instances by finding multiple seeds and resolving overlaps between components.
    
    :param features: Features are the input data used for clustering or segmentation, such as pixel
    values in an image or feature vectors in a dataset
    :param initial_seed: The `initial_seed` parameter is the starting point or seed node for the
    algorithm to identify connected components in the graph defined by the `adjacency` matrix. It serves
    as the initial reference point for the algorithm to begin the process of finding multiple instances
    or regions within the graph based on the provided features
    :param adjacency: Adjacency matrix represents the connections between nodes in a graph. In this
    context, it likely indicates the relationships between different regions or components in a dataset.
    The function `enhanced_instance_separation` seems to be aimed at separating instances or components
    based on features and adjacency information
    :param args: The `args` parameter in the `enhanced_instance_separation` function is used to pass
    additional arguments or configuration settings to the function. These arguments can be used to
    control the behavior of the function or adjust certain thresholds or parameters within the function
    logic
    :return: The function `enhanced_instance_separation` returns a list of connected components after
    resolving any overlaps between them. If there are multiple initial seeds, it finds connected
    components for each seed and then resolves overlaps between all pairs of components before returning
    the final list of components. If there is only one initial seed, it returns the connected component
    for that seed.
    """
    def find_multiple_seeds(features, adjacency, n_instances=2):
        """
        The function `find_multiple_seeds` takes features, adjacency matrix, and optional number of
        instances to find multiple seed nodes based on connectivity and degree threshold.
        
        :param features: Features are the input data used in the function. It could be a tensor
        containing the features of each instance in a dataset
        :param adjacency: Adjacency matrix represents connections between nodes in a graph. In this
        context, it seems like the adjacency matrix is being used to find multiple seeds in a graph
        based on certain criteria
        :param n_instances: The `n_instances` parameter in the `find_multiple_seeds` function represents
        the number of seed nodes to be found in addition to the initial seed node. It determines how
        many additional seed nodes will be identified based on the adjacency matrix and the specified
        conditions in the function, defaults to 2 (optional)
        :return: The function `find_multiple_seeds` returns a list of seed nodes for a graph based on
        the input features, adjacency matrix, and number of instances specified.
        """
        seeds = [initial_seed]
        mask = torch.ones_like(adjacency, device=adjacency.device)
        
        # Mask out first seed's region
        initial_component = get_connected_component(adjacency, initial_seed)
        mask[initial_component] = 0
        
        # Find additional seeds
        for _ in range(n_instances - 1):
            degrees = (adjacency * mask).sum(dim=1)
            next_seed = degrees.argmin()
            
            if degrees[next_seed] < args.degree_threshold:
                seeds.append(next_seed)
                component = get_connected_component(adjacency, next_seed)
                mask[component] = 0
                
        return seeds
    
    def resolve_overlap(component1, component2, features):
        """
        The function `resolve_overlap` compares two components based on feature similarity scores and
        assigns overlapping regions based on cosine similarity.
        
        :param component1: Component 1 is a set representing the first component or region of interest.
        It could be a collection of data points, pixels, or any other elements that define a specific
        area
        :param component2: Component2 is a variable representing a set of components or regions in a
        system or dataset. It is used in the `resolve_overlap` function to compare with `component1` and
        identify overlapping regions
        :param features: Features is a tensor containing feature vectors for each region in the
        components
        :return: The function `resolve_overlap` returns the updated components `component1` and
        `component2` after resolving any overlap between them based on feature similarity scores.
        """
        overlap = component1 & component2
        if not overlap.any():
            return component1, component2
            
        # Compute feature similarity scores
        feat1 = features[component1 & ~overlap].mean(dim=0)
        feat2 = features[component2 & ~overlap].mean(dim=0)
        overlap_feats = features[overlap]
        
        # Assign overlapping regions
        sim1 = F.cosine_similarity(overlap_feats, feat1.unsqueeze(0), dim=1)
        sim2 = F.cosine_similarity(overlap_feats, feat2.unsqueeze(0), dim=1)
        mask = sim1 > sim2
        
        return update_components(component1, component2, overlap, mask)
    
    # Main processing
    seeds = find_multiple_seeds(features, adjacency)
    if len(seeds) > 1:
        components = []
        for seed in seeds:
            comp = get_connected_component(adjacency, seed)
            components.append(comp)
            
        # Resolve overlaps between all pairs
        for i in range(len(components)):
            for j in range(i+1, len(components)):
                components[i], components[j] = resolve_overlap(
                    components[i], components[j], features
                )
        
        return components
    return [get_connected_component(adjacency, initial_seed)]

def optimize_processing_pipeline(model, dataset, args):
    """
    The function `optimize_processing_pipeline` optimizes data loading, feature extraction, and batch
    processing for a given model and dataset while optionally caching features.
    
    :param model: The `model` parameter in the `optimize_processing_pipeline` function refers to the
    neural network model that will be used for feature extraction and processing during the pipeline
    optimization. This model is typically a deep learning model implemented using a framework like
    PyTorch or TensorFlow
    :param dataset: The `dataset` parameter in the `optimize_processing_pipeline` function refers to the
    dataset that contains the input data for processing. It is used to create a DataLoader for efficient
    data loading during the processing pipeline. The dataset typically contains the input samples that
    will be processed by the model
    :param args: The `args` parameter in the `optimize_processing_pipeline` function likely contains
    configuration settings or hyperparameters for the data loading, processing, and caching operations.
    It is used to customize the behavior of the pipeline based on the provided arguments
    :return: The `optimize_processing_pipeline` function returns a list of results obtained from
    processing batches of data using the provided model and dataset, with optimizations for data
    loading, feature extraction, processing, and caching based on the arguments passed to the function.
    """
    # Initialize efficient data loading
    dataloader = DataLoader(
        dataset,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        pin_memory=True,
        prefetch_factor=2,
        persistent_workers=True
    )
    
    # Initialize feature cache
    feature_cache = FeatureCache(max_size=1000)
    
    # Setup mixed precision
    scaler = torch.cuda.amp.GradScaler()
    
    results = []
    for batch in dataloader:
        # Mixed precision and memory-efficient processing
        with torch.cuda.amp.autocast():
            with torch.no_grad():
                # Extract features
                features = optimize_feature_extraction(model, batch)
                
                # Process batch
                batch_results = process_batch(
                    features,
                    batch_size=args.batch_size,
                    optimize_memory=True
                )
                
        # Cache features if needed
        if args.use_cache:
            for idx, feat in enumerate(features):
                feature_cache.put(batch['image_id'][idx], feat)
                
        results.extend(batch_results)
        
        # Clear cache
        torch.cuda.empty_cache()
        
    return results

def process_batch(features, batch_size, optimize_memory=True):
    """
    The function `process_batch` processes a batch of features, optionally optimizing memory usage by
    converting features to half precision and processing in smaller sub-batches to prevent out-of-memory
    errors.
    
    :param features: The `features` parameter is a tensor containing the input features to be processed
    in batches
    :param batch_size: The `batch_size` parameter in the `process_batch` function represents the total
    number of features that need to be processed in a single batch. It determines how many features will
    be processed together before splitting them into smaller sub-batches to prevent out-of-memory (OOM)
    errors
    :param optimize_memory: The `optimize_memory` parameter is a boolean flag that indicates whether
    memory optimization techniques should be applied during the batch processing. When set to `True`,
    the function will convert the input features to half precision using the `.half()` method to reduce
    memory usage, defaults to True (optional)
    :return: The function `process_batch` returns the results of processing the input features in
    smaller sub-batches, where each sub-batch is processed using the `lost_batch` function. The results
    from all sub-batches are combined and returned as a list.
    """
    if optimize_memory:
        features = features.half()  # Use half precision
        
    # Process in smaller sub-batches if needed
    sub_batch_size = min(batch_size, 16)  # Prevent OOM
    results = []
    
    for idx in range(0, batch_size, sub_batch_size):
        sub_features = features[idx:idx + sub_batch_size]
        sub_results = lost_batch(sub_features)
        results.extend(sub_results)
        
    return results

def validate_detections(predictions, features, attention_maps, args):
    """
    The function `validate_detections` filters out predictions based on quality scores computed from
    attention maps, features, and size ratio.
    
    :param predictions: Predictions are the output of a detection model, typically bounding boxes or
    segmentation masks that represent objects detected in an image
    :param features: Features are the extracted visual features from the input data, which are used in
    the detection process. They can include information such as shapes, colors, textures, or any other
    relevant visual characteristics of the objects being detected
    :param attention_maps: Attention maps are visualizations that show which parts of an input image are
    attended to by a neural network model when making predictions. They are often used in tasks such as
    image captioning, visual question answering, and object detection to understand where the model is
    focusing its attention. In the `validate_detections
    :param args: The `args` parameter in the `validate_detections` function likely contains various
    arguments or settings that are used within the function. These arguments could include parameters
    such as `quality_threshold`, which is used to filter out predictions based on their quality score
    :return: The function `validate_detections` returns a list of valid predictions that meet the
    quality threshold specified in the `args` argument.
    """
    valid_predictions = []
    
    for pred, feat, attn in zip(predictions, features, attention_maps):
        # Compute quality scores
        coverage_score = compute_coverage_score(pred, attn)
        feature_consistency = compute_feature_consistency(pred, feat)
        size_ratio = compute_size_ratio(pred)
        
        # Combined quality score
        quality_score = (
            coverage_score * 0.4 +
            feature_consistency * 0.4 +
            size_ratio * 0.2
        )
        
        if quality_score > args.quality_threshold:
            valid_predictions.append(pred)
            
    return valid_predictions

def lost(feats, dims, scales, init_image_size, k_patches=100):
    """
    Implementation of LOST method.
    Inputs
        feats: the pixel/patche features of an image
        dims: dimension of the map from which the features are used
        scales: from image to map scale
        init_image_size: size of the image
        k_patches: number of k patches retrieved that are compared to the seed at seed expansion
    Outputs
        pred: box predictions
        A: binary affinity matrix
        scores: lowest degree scores for all patches
        seed: selected patch corresponding to an object
    """
    # Compute the similarity
    A = (feats @ feats.transpose(1, 2)).squeeze()

    # Compute the inverse degree centrality measure per patch
    sorted_patches, scores = patch_scoring(A)

    # Select the initial seed
    seed = sorted_patches[0]

    # Seed expansion
    potentials = sorted_patches[:k_patches]
    similars = potentials[A[seed, potentials] > 0.0]
    M = torch.sum(A[similars, :], dim=0)

    # Box extraction
    pred, _ = detect_box(
        M, seed, dims, scales=scales, initial_im_size=init_image_size[1:]
    )

    return np.asarray(pred), A, scores, seed


def patch_scoring(M, threshold=0.):
    """
    Patch scoring based on the inverse degree.
    """
    # Cloning important
    A = M.clone()

    # Zero diagonal
    A.fill_diagonal_(0)

    # Make sure symmetric and non nul
    A[A < 0] = 0
    C = A + A.t()

    # Sort pixels by inverse degree
    cent = -torch.sum(A > threshold, dim=1).type(torch.float32)
    sel = torch.argsort(cent, descending=True)

    return sel, cent


def detect_box(A, seed, dims, initial_im_size=None, scales=None):
    """
    Extract a box corresponding to the seed patch. Among connected components extract from the affinity matrix, select the one corresponding to the seed patch.
    """
    w_featmap, h_featmap = dims

    correl = A.reshape(w_featmap, h_featmap).float()

    # Compute connected components
    labeled_array, num_features = scipy.ndimage.label(correl.cpu().numpy() > 0.0)

    # Find connected component corresponding to the initial seed
    cc = labeled_array[np.unravel_index(seed.cpu().numpy(), (w_featmap, h_featmap))]

    # Should not happen with LOST
    if cc == 0:
        raise ValueError("The seed is in the background component.")

    # Find box
    mask = np.where(labeled_array == cc)
    # Add +1 because excluded max
    ymin, ymax = min(mask[0]), max(mask[0]) + 1
    xmin, xmax = min(mask[1]), max(mask[1]) + 1

    # Rescale to image size
    r_xmin, r_xmax = scales[1] * xmin, scales[1] * xmax
    r_ymin, r_ymax = scales[0] * ymin, scales[0] * ymax

    pred = [r_xmin, r_ymin, r_xmax, r_ymax]

    # Check not out of image size (used when padding)
    if initial_im_size:
        pred[2] = min(pred[2], initial_im_size[1])
        pred[3] = min(pred[3], initial_im_size[0])

    # Coordinate predictions for the feature space
    # Axis different then in image space
    pred_feats = [ymin, xmin, ymax, xmax]

    return pred, pred_feats


def dino_seg(attn, dims, patch_size, head=0):
    """
    Extraction of boxes based on the DINO segmentation method proposed in https://github.com/facebookresearch/dino. 
    Modified from https://github.com/facebookresearch/dino/blob/main/visualize_attention.py
    """
    w_featmap, h_featmap = dims
    nh = attn.shape[1]
    official_th = 0.6

    # We keep only the output patch attention
    # Get the attentions corresponding to [CLS] token
    attentions = attn[0, :, 0, 1:].reshape(nh, -1)

    # we keep only a certain percentage of the mass
    val, idx = torch.sort(attentions)
    val /= torch.sum(val, dim=1, keepdim=True)
    cumval = torch.cumsum(val, dim=1)
    th_attn = cumval > (1 - official_th)
    idx2 = torch.argsort(idx)
    for h in range(nh):
        th_attn[h] = th_attn[h][idx2[h]]
    th_attn = th_attn.reshape(nh, w_featmap, h_featmap).float()

    # Connected components
    labeled_array, num_features = scipy.ndimage.label(th_attn[head].cpu().numpy())

    # Find the biggest component
    size_components = [np.sum(labeled_array == c) for c in range(np.max(labeled_array))]

    if len(size_components) > 1:
        # Select the biggest component avoiding component 0 corresponding to background
        biggest_component = np.argmax(size_components[1:]) + 1
    else:
        # Cases of a single component
        biggest_component = 0

    # Mask corresponding to connected component
    mask = np.where(labeled_array == biggest_component)

    # Add +1 because excluded max
    ymin, ymax = min(mask[0]), max(mask[0]) + 1
    xmin, xmax = min(mask[1]), max(mask[1]) + 1

    # Rescale to image
    r_xmin, r_xmax = xmin * patch_size, xmax * patch_size
    r_ymin, r_ymax = ymin * patch_size, ymax * patch_size
    pred = [r_xmin, r_ymin, r_xmax, r_ymax]

    return pred
